{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3**\n",
        "\n",
        "The task was to solve the XOR problem using a NN built from scratch.\n",
        "\n",
        "In order to evaluate how NN's work and solve the XOR problem, NN's with two inputs, two hidden nodes and an output node with bias nodes (initiated with value of 1) being connected to the non-input nodes, were built. Each NN was trained on 1000 iterations using the Gradient Descent algorithm. Learning rates of 0.001, 0.01, 0.1 and 1 were chosen.\n",
        "\n",
        "The weights for non-bias nodes were initiated manually using only 1's or 0's, 0's and 1's, only 0.5's or with increasing numbers between 0 and 1.\n",
        "\n",
        "Using sigmoid, tanh or relu activation functions for hidden and output nodes: None of the manually chosen weights solved the XOR problem sufficiently. Learning rates of 0.1 or 1 reduced MSE most for sigmoid function NN's, using tanh activation functions, MSE's may be smaller for 0.01, 0.1 or 1 learning rates, depending on the inital weights. For relu, learning rate of 1 performed worst regarding MSE.\n",
        "\n",
        "Using the lazy approach of randomly initiating weights between 0 and 1 for non-bias nodes in NN's with sigmoid activation functions, 15% out of 20 runs contained weights which solved the XOR problem. Tanh and relu functions both did not find optimal solutions to the XOR problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tfx2Hdl7_Fe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(3311791)"
      ],
      "metadata": {
        "id": "tNODr9S0WQKv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def xor_net(inputs, weights):\n",
        "    if len(inputs) != 2 or len(weights) != 9:\n",
        "        raise ValueError(\"inputs length has to be 2 and weights length has to be 9.\")\n",
        "\n",
        "    input1, input2 = inputs\n",
        "    w_input_hidden1 = weights[:2]\n",
        "    w_input_hidden2 = weights[2:4]\n",
        "    w_hidden_output1 = weights[4:6]\n",
        "    w_bias_hidden1 = weights[6:8]\n",
        "    w_bias_output = weights[8]\n",
        "\n",
        "    hidden1 = sigmoid(input1 * w_input_hidden1[0] + input2 * w_input_hidden1[1] + 1 * w_bias_hidden1[0])\n",
        "    hidden2 = sigmoid(input1 * w_input_hidden2[0] + input2 * w_input_hidden2[1] + 1 * w_bias_hidden1[1])\n",
        "    output = sigmoid(hidden1 * w_hidden_output1[0] + hidden2 * w_hidden_output1[1] + 1 * w_bias_output)\n",
        "    return output\n",
        "\n",
        "def mse(weights):\n",
        "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "    targets = [0, 1, 1, 0]\n",
        "    mse_sum = 0\n",
        "    network_outputs = []\n",
        "\n",
        "    for i in range(4):\n",
        "        output = xor_net(inputs[i], weights)\n",
        "        network_outputs.append(output)\n",
        "        mse_sum += (output - targets[i]) ** 2\n",
        "    return mse_sum / 4, network_outputs\n",
        "\n",
        "def grdmse(weights, learning_rate):\n",
        "    grad = np.zeros_like(weights)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights_plus = np.copy(weights)\n",
        "        weights_minus = np.copy(weights)\n",
        "\n",
        "        weights_plus[i] += learning_rate\n",
        "        weights_minus[i] -= learning_rate\n",
        "\n",
        "        grad[i] = (mse(weights_plus)[0] - mse(weights_minus)[0]) / (2 * learning_rate)\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(weights, learning_rate, num_iterations):\n",
        "    for _ in range(num_iterations):\n",
        "        grad = grdmse(weights, learning_rate)\n",
        "        weights = weights - learning_rate * grad\n",
        "    return weights\n",
        "\n",
        "def train(weights):\n",
        "    for i in range(len(learning_rates)):\n",
        "        trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "\n",
        "        for input_pair in inputs:\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "        print(f\"Weights initilization: {weights}, learning rate: {learning_rates[i]} output: {binary_outputs}, MSE: {mse_val:.4f}\")\n",
        "\n",
        "    return binary_outputs\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "num_iterations = 1000\n",
        "inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "\n",
        "print(\"Sigmoid activation function\")\n",
        "train([1] * 9)\n",
        "train(np.concatenate([[0] * 6, [1] * 3]))\n",
        "train([0, 1, 0, 1, 0, 1, 1, 1, 1])\n",
        "train(np.concatenate([[0.5] * 6, [1] * 3]))\n",
        "train([.25, .33, .44, .55, .65, .70, 1, 1, 1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y9tJQwckKtw",
        "outputId": "6eaed184-29ae-415c-8775-0711ceb8b8c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid activation function\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.4258\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.2746\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.1 output: [0, 1, 1, 1], MSE: 0.2473\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.4388\n",
            "Weights initilization: [0 0 0 0 0 0 1 1 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.2858\n",
            "Weights initilization: [0 0 0 0 0 0 1 1 1], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.2505\n",
            "Weights initilization: [0 0 0 0 0 0 1 1 1], learning rate: 0.1 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0 0 0 0 0 0 1 1 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.3034\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 1, 1, 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.3524\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 1, 1, 1], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.2520\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 1, 1, 1], learning rate: 0.1 output: [0, 0, 1, 0], MSE: 0.2482\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 1, 1, 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.3787\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 1.  1.  1. ], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.3597\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 1.  1.  1. ], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.2527\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 1.  1.  1. ], learning rate: 0.1 output: [1, 1, 1, 0], MSE: 0.2502\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 1.  1.  1. ], learning rate: 1 output: [0, 1, 1, 1], MSE: 0.2475\n",
            "Weights initilization: [0.25, 0.33, 0.44, 0.55, 0.65, 0.7, 1, 1, 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.3854\n",
            "Weights initilization: [0.25, 0.33, 0.44, 0.55, 0.65, 0.7, 1, 1, 1], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.2559\n",
            "Weights initilization: [0.25, 0.33, 0.44, 0.55, 0.65, 0.7, 1, 1, 1], learning rate: 0.1 output: [1, 1, 1, 1], MSE: 0.2500\n",
            "Weights initilization: [0.25, 0.33, 0.44, 0.55, 0.65, 0.7, 1, 1, 1], learning rate: 1 output: [0, 1, 1, 1], MSE: 0.1982\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lazy(num_trials):\n",
        "    num_runs = 0\n",
        "    for trial in range(num_trials):\n",
        "        for i in range(len(learning_rates)):\n",
        "            weights = np.concatenate([np.random.rand(6), [1] * 3])\n",
        "            trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "            num_runs = num_runs + 1\n",
        "            if binary_outputs == [0, 1, 1, 0]:\n",
        "                successful_weights = [trained_weights]\n",
        "                return successful_weights, num_runs\n",
        "    return None, num_runs\n",
        "\n",
        "def lazy_repeat(num_trials):\n",
        "    c = 0\n",
        "    num_success = 0\n",
        "    for trial in range(num_trials):\n",
        "        successful_weights, num_runs = lazy(1)\n",
        "        c = c + num_runs\n",
        "        if successful_weights:\n",
        "            num_success = num_success + 1\n",
        "    #num_trained = num_trials *\n",
        "    prop = num_success / c\n",
        "    print(f\"Number of trained NNs: {c}, Solutions to XOR: {num_success}, Proportion: {prop}\")\n",
        "    return prop\n",
        "\n",
        "xor_correct_weight, _ = lazy(5)\n",
        "lazy_repeat(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTDwi_pSdnrS",
        "outputId": "bbb2c0ea-812a-4d1c-d263-ea1f6949dc27"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trained NNs: 20, Solutions to XOR: 3, Proportion: 0.15\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tanh activation function\n",
        "def tanh(x):\n",
        "    return (np.tanh(x) + 1) /2 # scale tanh's [-1, 1] to [0, 1]\n",
        "\n",
        "def xor_net(inputs, weights):\n",
        "    if len(inputs) != 2 or len(weights) != 9:\n",
        "        raise ValueError(\"inputs length has to be 2 and weights length has to be 9.\")\n",
        "\n",
        "    input1, input2 = inputs\n",
        "    w_input_hidden1 = weights[:2]\n",
        "    w_input_hidden2 = weights[2:4]\n",
        "    w_hidden_output1 = weights[4:6]\n",
        "    w_bias_hidden1 = weights[6:8]\n",
        "    w_bias_output = weights[8]\n",
        "\n",
        "    hidden1 = tanh(input1 * w_input_hidden1[0] + input2 * w_input_hidden1[1] + 1 * w_bias_hidden1[0])\n",
        "    hidden2 = tanh(input1 * w_input_hidden2[0] + input2 * w_input_hidden2[1] + 1 * w_bias_hidden1[1])\n",
        "    output = tanh(hidden1 * w_hidden_output1[0] + hidden2 * w_hidden_output1[1] + 1 * w_bias_output)\n",
        "    return output\n",
        "\n",
        "def mse(weights):\n",
        "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "    targets = [0, 1, 1, 0]\n",
        "    mse_sum = 0\n",
        "    network_outputs = []\n",
        "\n",
        "    for i in range(4):\n",
        "        output = xor_net(inputs[i], weights)\n",
        "        network_outputs.append(output)\n",
        "        mse_sum += (output - targets[i]) ** 2\n",
        "    return mse_sum / 4, network_outputs\n",
        "\n",
        "def grdmse(weights, learning_rate):\n",
        "    grad = np.zeros_like(weights)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights_plus = np.copy(weights)\n",
        "        weights_minus = np.copy(weights)\n",
        "\n",
        "        weights_plus[i] += learning_rate\n",
        "        weights_minus[i] -= learning_rate\n",
        "\n",
        "        grad[i] = (mse(weights_plus)[0] - mse(weights_minus)[0]) / (2 * learning_rate)\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(weights, learning_rate, num_iterations):\n",
        "    for _ in range(num_iterations):\n",
        "        grad = grdmse(weights, learning_rate)\n",
        "        weights = weights - learning_rate * grad\n",
        "    return weights\n",
        "\n",
        "def train(weights):\n",
        "    for i in range(len(learning_rates)):\n",
        "        trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "\n",
        "        for input_pair in inputs:\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "        print(f\"Weights initilization: {weights}, learning rate: {learning_rates[i]} output: {binary_outputs}, MSE: {mse_val:.4f}\")\n",
        "\n",
        "    weights = np.concatenate((np.random.rand(7), np.array([1] * 2)))\n",
        "print(\"Tanh activation function\")\n",
        "train([1] * 9)\n",
        "train(np.concatenate([[0] * 7, [1] * 2]))\n",
        "train([0, 1, 0, 1, 0, 1, 0, 1, 1])\n",
        "train(np.concatenate([[0.5] * 7, [1] * 2]))\n",
        "\n",
        "def lazy(num_trials):\n",
        "    num_runs = 0\n",
        "    for trial in range(num_trials):\n",
        "        for i in range(len(learning_rates)):\n",
        "            weights = np.concatenate([np.random.rand(6), [1] * 3])\n",
        "            trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "            num_runs = num_runs + 1\n",
        "            if binary_outputs == [0, 1, 1, 0]:\n",
        "                successful_weights = [trained_weights]\n",
        "                return successful_weights, num_runs\n",
        "    return None, num_runs\n",
        "\n",
        "def lazy_repeat(num_trials):\n",
        "    c = 0\n",
        "    num_success = 0\n",
        "    for trial in range(num_trials):\n",
        "        successful_weights, num_runs = lazy(1)\n",
        "        c = c + num_runs\n",
        "        if successful_weights:\n",
        "            num_success = num_success + 1\n",
        "    #num_trained = num_trials *\n",
        "    prop = num_success / c\n",
        "    print(f\"Number of trained NNs: {c}, Solutions to XOR: {num_success}, Proportion: {prop}\")\n",
        "    return prop\n",
        "\n",
        "xor_correct_weight, _ = lazy(5)\n",
        "lazy_repeat(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrJTuox1B6Al",
        "outputId": "40f7dcff-518c-4e8c-835b-909ed55a3ed5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanh activation function\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.4965\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.01 output: [1, 1, 1, 1], MSE: 0.4950\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.1 output: [0, 1, 1, 1], MSE: 0.2358\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.4968\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.3127\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.01 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.1 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.3950\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.4701\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.01 output: [1, 0, 1, 0], MSE: 0.2506\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.1 output: [0, 0, 1, 0], MSE: 0.1770\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 1 output: [1, 1, 1, 1], MSE: 0.4798\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.4720\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.01 output: [1, 1, 1, 0], MSE: 0.2518\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.1 output: [0, 1, 1, 1], MSE: 0.2402\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 1 output: [0, 1, 1, 1], MSE: 0.1586\n",
            "Number of trained NNs: 20, Solutions to XOR: 0, Proportion: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relu activation function\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def xor_net(inputs, weights):\n",
        "    if len(inputs) != 2 or len(weights) != 9:\n",
        "        raise ValueError(\"inputs length has to be 2 and weights length has to be 9.\")\n",
        "\n",
        "    input1, input2 = inputs\n",
        "    w_input_hidden1 = weights[:2]\n",
        "    w_input_hidden2 = weights[2:4]\n",
        "    w_hidden_output1 = weights[4:6]\n",
        "    w_bias_hidden1 = weights[6:8]\n",
        "    w_bias_output = weights[8]\n",
        "\n",
        "    hidden1 = relu(input1 * w_input_hidden1[0] + input2 * w_input_hidden1[1] + 1 * w_bias_hidden1[0])\n",
        "    hidden2 = relu(input1 * w_input_hidden2[0] + input2 * w_input_hidden2[1] + 1 * w_bias_hidden1[1])\n",
        "    output = relu(hidden1 * w_hidden_output1[0] + hidden2 * w_hidden_output1[1] + 1 * w_bias_output)\n",
        "    return output\n",
        "\n",
        "def mse(weights):\n",
        "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "    targets = [0, 1, 1, 0]\n",
        "    mse_sum = 0\n",
        "    network_outputs = []\n",
        "\n",
        "    for i in range(4):\n",
        "        output = xor_net(inputs[i], weights)\n",
        "        network_outputs.append(output)\n",
        "        mse_sum += (output - targets[i]) ** 2\n",
        "    return mse_sum / 4, network_outputs\n",
        "\n",
        "def grdmse(weights, learning_rate):\n",
        "    grad = np.zeros_like(weights)\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights_plus = np.copy(weights)\n",
        "        weights_minus = np.copy(weights)\n",
        "\n",
        "        weights_plus[i] += learning_rate\n",
        "        weights_minus[i] -= learning_rate\n",
        "\n",
        "        grad[i] = (mse(weights_plus)[0] - mse(weights_minus)[0]) / (2 * learning_rate)\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(weights, learning_rate, num_iterations):\n",
        "    for _ in range(num_iterations):\n",
        "        grad = grdmse(weights, learning_rate)\n",
        "        weights = weights - learning_rate * grad\n",
        "    return weights\n",
        "\n",
        "def train(weights):\n",
        "    for i in range(len(learning_rates)):\n",
        "        trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "\n",
        "        for input_pair in inputs:\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "        print(f\"Weights initilization: {weights}, learning rate: {learning_rates[i]} output: {binary_outputs}, MSE: {mse_val:.4f}\")\n",
        "\n",
        "    weights = np.concatenate((np.random.rand(7), np.array([1] * 2)))\n",
        "\n",
        "print(\"Relu activation function\")\n",
        "train([1] * 9)\n",
        "train(np.concatenate([[0] * 7, [1] * 2]))\n",
        "train([0, 1, 0, 1, 0, 1, 0, 1, 1])\n",
        "train(np.concatenate([[0.5] * 7, [1] * 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "821X1eTcJeaB",
        "outputId": "15163ba0-8c99-4feb-e5c7-cb2c9c18550c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relu activation function\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.001 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.01 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 0.1 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [1, 1, 1, 1, 1, 1, 1, 1, 1], learning rate: 1 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.001 output: [1, 1, 1, 1], MSE: 0.2501\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.01 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 0.1 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0 0 0 0 0 0 0 1 1], learning rate: 1 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.001 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.01 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 0.1 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0, 1, 0, 1, 0, 1, 0, 1, 1], learning rate: 1 output: [0, 0, 0, 0], MSE: 0.5000\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.001 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.01 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 0.1 output: [1, 1, 1, 0], MSE: 0.2500\n",
            "Weights initilization: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.  1. ], learning rate: 1 output: [0, 0, 0, 0], MSE: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lazy(num_trials):\n",
        "    num_runs = 0\n",
        "    for trial in range(num_trials):\n",
        "        for i in range(len(learning_rates)):\n",
        "            weights = np.concatenate([np.random.rand(6), [1] * 3])\n",
        "            trained_weights = gradient_descent(weights, learning_rates[i], num_iterations)\n",
        "            mse_val, outputs = mse(trained_weights)\n",
        "            binary_outputs = [1 if output > 0.5 else 0 for output in outputs]\n",
        "            num_runs = num_runs + 1\n",
        "            if binary_outputs == [0, 1, 1, 0]:\n",
        "                successful_weights = [trained_weights]\n",
        "                return successful_weights, num_runs\n",
        "    return None, num_runs\n",
        "\n",
        "def lazy_repeat(num_trials):\n",
        "    c = 0\n",
        "    num_success = 0\n",
        "    for trial in range(num_trials):\n",
        "        successful_weights, num_runs = lazy(1)\n",
        "        c = c + num_runs\n",
        "        if successful_weights:\n",
        "            num_success = num_success + 1\n",
        "    #num_trained = num_trials *\n",
        "    prop = num_success / c\n",
        "    print(f\"Number of trained NNs: {c}, Solutions to XOR: {num_success}, Proportion: {prop}\")\n",
        "    return prop\n",
        "\n",
        "xor_correct_weight, _ = lazy(5)\n",
        "lazy_repeat(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ5CZoLELqfM",
        "outputId": "b7a2a229-cadc-4642-830d-9256af5102c6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trained NNs: 20, Solutions to XOR: 0, Proportion: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}